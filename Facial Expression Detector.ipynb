{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook details a deep learning approach to image modeling using keras and tensorflow on the backend. Keras is a high-level API to build and train deep learning models. It's used for fast prototyping, advanced research, and productions. TensorFlow is a free and open-source software library for dataflow and differentiable programming across a range of tasks. It is a symbolic math library, and is also used for machine learning applications such as neural networks. Using keras and tensorflow together provides us with a simplified method of developing an image classification model. The general process to building this detector is as follows:\n",
    "\n",
    "1. Obtain labeled data \n",
    "2. Identify the structure of our image data \n",
    "3. Prepare the data for modeling\n",
    "4. Select the appropriate model parameters\n",
    "5. Build the model \n",
    "6. Validate the model using metrics\n",
    "7. Evaluate predictions \n",
    "\n",
    "Before we begin, we need to load the appropriate libraries needed to run the notenook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Load Libraries\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from random import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers. normalization import BatchNormalization\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing import image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain Data \n",
    "\n",
    "After loading the libraries, we then need to load the training and testing data. This data was sourced from a kaggle competition located here: https://www.kaggle.com/c/emotion-detection-from-facial-expressions/overview. The raw data can be found here: https://github.com/muxspace/facial_expressions. Due to the size limitations of github, the images will have to be stored in a separate location not contained in this repository. To run this notebook we will need to specify the directory of the images folder and the path to the csv file containing the labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify data locations\n",
    "IMAGES = \"/Users/chad/Projects/large-data-files/Emotion-Detecion/images\" # Folder containing images\n",
    "LABELS = '/Users/chad/Projects/large-data-files/Emotion-Detecion/data/legend.csv' # CSV with image file names and correct labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify the structure of our image data\n",
    "\n",
    "The data is provided in two forms. First, there is a folder of the raw images to be processed. The second is a .csv file containing image labels for each file name in the images folder. In order to build this model, we need to load the images and append the appropriate labels to the data. Before we load the data, we want to know how we should format the height x width image data dimensions for inputting to a keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Height: 335.75410075089303\n",
      "Max Height: 536\n",
      "Min Height: 24\n",
      "\n",
      "\n",
      "Average Width: 333.5470583946927\n",
      "Max Width: 441\n",
      "Min Width: 18\n"
     ]
    }
   ],
   "source": [
    "# Want to know how we should format the height x width image data dimensions\n",
    "# for inputting to a keras model\n",
    "def get_size_statistics():\n",
    "    heights = []\n",
    "    widths = []\n",
    "    img_count = 0\n",
    "    for img in os.listdir(IMAGES):\n",
    "        try:\n",
    "            path = os.path.join(IMAGES, img)\n",
    "            if \"DS_Store\" not in path:\n",
    "                data = np.array(Image.open(path))\n",
    "                heights.append(data.shape[0])\n",
    "                widths.append(data.shape[1])\n",
    "                img_count += 1\n",
    "        except:\n",
    "            continue\n",
    "    avg_height = sum(heights) / len(heights)\n",
    "    avg_width = sum(widths) / len(widths)\n",
    "    print(\"Average Height: \" + str(avg_height))\n",
    "    print(\"Max Height: \" + str(max(heights)))\n",
    "    print(\"Min Height: \" + str(min(heights)))\n",
    "    print('\\n')\n",
    "    print(\"Average Width: \" + str(avg_width))\n",
    "    print(\"Max Width: \" + str(max(widths)))\n",
    "    print(\"Min Width: \" + str(min(widths)))\n",
    "\n",
    "get_size_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the analysis show that our average image height is 335 and our average image width is 333. This means that when we fit our model we can select dimensions as high as these values and should not run into any major issues. I have already experimented with several data dimensions and determined that using lower values does not degrade model performance whereas using high values exponentially increased training time. As such, for the purpose of this analysis I will use a 30 x 30 data dimension when loading the images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data for modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for this analysis is contained within one folder. This means that before building the training data, we should set aside a subset of images to evaluate our model predictions after we build the model. The code below first loads the labeled csv file and cleans up the label (converts all strings to lower case for consistency). We then create a new column of integer codes based on this label. This is important as the keras to_categorical function requires an integer value to work. We then take a randomly selected 10% subset of this data frame for later testing. The test records are then removed from the training dataset and we reset the index of both data frames to account for this reduction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(LABELS)\n",
    "train.emotion = train.emotion.apply(lambda x: x.lower())\n",
    "train.emotion = pd.Categorical(train.emotion)\n",
    "train['code'] = train.emotion.cat.codes\n",
    "test = train.sample(frac=0.1, replace=True, random_state=1) # take 10% of records to make predictions later\n",
    "train = pd.concat([train, test]).drop_duplicates(keep=False).reset_index() # remove test from training set\n",
    "test = test.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the image function from keras preprocessing, we then load the images with the specified dimensions above (30 x 30 x 1) and convert them to an array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12402/12402 [00:08<00:00, 1488.58it/s]\n"
     ]
    }
   ],
   "source": [
    "train_image = []\n",
    "for i in tqdm(range(train.shape[0])):\n",
    "    img = image.load_img(IMAGES+'/'+train['image'][i], target_size=(30,30,1), color_mode = \"grayscale\")\n",
    "    img = image.img_to_array(img)\n",
    "    img = img/255\n",
    "    train_image.append(img)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last portion of the data preparation involves us converting the multi class labels into categorical columns that will be used during training and then creating a training set and validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(train_image)\n",
    "y = train['code'].values\n",
    "y = to_categorical(y) # dummie code\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) # takes 20% of records for y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select the appropriate model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a simple architecture with 2 convolutional layers, one dense hidden layer and an output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=(30,30,1)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(8, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9921 samples, validate on 2481 samples\n",
      "Epoch 1/5\n",
      "9921/9921 [==============================] - 28s 3ms/step - loss: 0.9716 - acc: 0.6011 - val_loss: 0.6793 - val_acc: 0.7674\n",
      "Epoch 2/5\n",
      "9921/9921 [==============================] - 24s 2ms/step - loss: 0.7027 - acc: 0.7626 - val_loss: 0.5935 - val_acc: 0.7940\n",
      "Epoch 3/5\n",
      "9921/9921 [==============================] - 24s 2ms/step - loss: 0.6299 - acc: 0.7856 - val_loss: 0.5556 - val_acc: 0.8110\n",
      "Epoch 4/5\n",
      "9696/9921 [============================>.] - ETA: 0s - loss: 0.5892 - acc: 0.7997"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate predictions on the randomly sampled 10% of images in the data folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = []\n",
    "for i in tqdm(range(test.shape[0])):\n",
    "    img = image.load_img(IMAGES+'/'+test['image'][i], target_size=(30,30,1), color_mode = \"grayscale\")\n",
    "    img = image.img_to_array(img)\n",
    "    img = img/255\n",
    "    test_image.append(img)\n",
    "test_data = np.array(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making predictions\n",
    "test['predicted'] = model.predict_classes(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predicted emotion column\n",
    "ids = test[['emotion', 'code']].drop_duplicates()\n",
    "ids.columns = ['predicted_emotion', 'code']\n",
    "test = pd.merge(test, ids, how='left', left_on='predicted', right_on='code')\n",
    "test = test[['user.id', 'image', 'emotion', 'code_x', 'predicted', 'predicted_emotion']]\n",
    "test.columns =['user.id', 'image', 'emotion', 'emotion_code', 'predicted_code', 'predicted_emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = test['predicted_code']\n",
    "y_true = test['emotion_code']\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to print image, true label, and prediction\n",
    "\n",
    "def print_image(index):\n",
    "    img = image.load_img(IMAGES+'/'+test['image'][index], target_size=(300,300,1), color_mode = \"grayscale\")\n",
    "    plt.imshow(img, cmap = 'gist_gray')\n",
    "    print('Model predicted ' + test['predicted_emotion'][index] + ' the true emotion is ' + test['emotion'][index])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_image(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_image(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_image(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_image(345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_image(88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
